{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import numpy\n",
    "import math\n",
    "from aips import get_engine\n",
    "from aips.data_loaders.outdoors import load_dataframe\n",
    "from pyspark import SparkConf\n",
    "from sentence_transformers import SentenceTransformer, SimilarityFunction\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "engine = get_engine()\n",
    "#Recommended for making ALS run faster, if you have enough memory / cores allocated to docker\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.driver.memory\", \"8g\")\n",
    "conf.set(\"spark.executor.memory\", \"8g\")\n",
    "conf.set(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "conf.set(\"spark.dynamicAllocation.executorMemoryOverhead\", \"8g\")\n",
    "spark = SparkSession.builder.appName(\"AIPS-ch13\").config(conf=conf).getOrCreate()\n",
    "\n",
    "model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\",\n",
    "                            similarity_fn_name=SimilarityFunction.DOT_PRODUCT,\n",
    "                            truncate_dim=1024)\n",
    "\n",
    "#https://github.com/facebookresearch/faiss/wiki/Pre--and-post-processing\n",
    "#https://github.com/facebookresearch/faiss/wiki\n",
    "#https://huggingface.co/spaces/sentence-transformers/quantized-retrieval/blob/main/app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n",
      "README.md\n",
      "concepts.pickle\n",
      "._guesses.csv\n",
      "guesses.csv\n",
      "._guesses_all.json\n",
      "guesses_all.json\n",
      "outdoors_concepts.pickle\n",
      "outdoors_embeddings.pickle\n",
      "._outdoors_golden_answers.csv\n",
      "outdoors_golden_answers.csv\n",
      "._outdoors_golden_answers.xlsx\n",
      "outdoors_golden_answers.xlsx\n",
      "._outdoors_golden_answers_20210130.csv\n",
      "outdoors_golden_answers_20210130.csv\n",
      "outdoors_labels.pickle\n",
      "outdoors_question_answering_contexts.json\n",
      "outdoors_questionanswering_test_set.json\n",
      "outdoors_questionanswering_train_set.json\n",
      "._posts.csv\n",
      "posts.csv\n",
      "predicates.pickle\n",
      "pull_aips_dependency.py\n",
      "._question-answer-seed-contexts.csv\n",
      "question-answer-seed-contexts.csv\n",
      "question-answer-squad2-guesses.csv\n",
      "._roberta-base-squad2-outdoors\n",
      "roberta-base-squad2-outdoors/\n",
      "roberta-base-squad2-outdoors/._tokenizer_config.json\n",
      "roberta-base-squad2-outdoors/tokenizer_config.json\n",
      "roberta-base-squad2-outdoors/._special_tokens_map.json\n",
      "roberta-base-squad2-outdoors/special_tokens_map.json\n",
      "roberta-base-squad2-outdoors/._config.json\n",
      "roberta-base-squad2-outdoors/config.json\n",
      "roberta-base-squad2-outdoors/._merges.txt\n",
      "roberta-base-squad2-outdoors/merges.txt\n",
      "roberta-base-squad2-outdoors/._training_args.bin\n",
      "roberta-base-squad2-outdoors/training_args.bin\n",
      "roberta-base-squad2-outdoors/._pytorch_model.bin\n",
      "roberta-base-squad2-outdoors/pytorch_model.bin\n",
      "roberta-base-squad2-outdoors/._vocab.json\n",
      "roberta-base-squad2-outdoors/vocab.json\n"
     ]
    }
   ],
   "source": [
    "![ ! -d 'outdoors' ] && git clone --depth=1 https://github.com/ai-powered-search/outdoors.git\n",
    "! cd outdoors && git pull\n",
    "! cd outdoors && cat outdoors.tgz.part* > outdoors.tgz\n",
    "! cd outdoors && mkdir -p '../data/outdoors/' && tar -xvf outdoors.tgz -C '../data/outdoors/'\n",
    "\n",
    "#outdoors_collection = engine.create_collection(\"outdoors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: I updated the get_embeddings method earlier in the chapter to have the same method signature, so no need to duplicate it in the manuscript. Ideally we'd load this in from a python file for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.stop_multi_process_pool(pool)\n",
    "#pool = model.start_multi_process_pool()\n",
    "#embeddings = model.encode(texts, convert_to_tensor=False).tolist()\n",
    "\n",
    "def get_embeddings(texts, model, cache_name, ignore_cache=False):\n",
    "    cache_file_name = f\"data/embeddings/{cache_name}.pickle\"\n",
    "    if ignore_cache or not os.path.isfile(cache_file_name):        \n",
    "        embeddings = model.encode(texts, normalize_embeddings=True)\n",
    "        os.makedirs(os.path.dirname(cache_file_name), exist_ok=True)\n",
    "        with open(cache_file_name, \"wb\") as fd:\n",
    "            pickle.dump(embeddings, fd)\n",
    "    else:\n",
    "        with open(cache_file_name, \"rb\") as fd:\n",
    "            embeddings = pickle.load(fd)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate code for Quantization listings\n",
    "### Generating embeddings and benchmark data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from aips.data_loaders.outdoors import load_dataframe\n",
    "\n",
    "def display_results(scores, ids, data):\n",
    "    results = generate_search_results(scores, ids, data)\n",
    "    display(results)\n",
    "    return results\n",
    "\n",
    "def get_outdoors_data():\n",
    "    outdoors_dataframe = load_dataframe(\"data/outdoors/posts.csv\")\n",
    "    outdoors_data = list(outdoors_dataframe.rdd.map(lambda r: r.asDict()).collect())\n",
    "    return outdoors_data\n",
    "\n",
    "def display_statistics(full_search_results, quantized_search_results, start_message=\"Recall\"):\n",
    "    index_name = quantized_search_results[\"index_name\"]\n",
    "    full_search_time = full_search_results[\"time_taken\"]\n",
    "    time_taken = quantized_search_results[\"time_taken\"]\n",
    "    time_imp = round((full_search_time - time_taken) * 100 / full_search_time, 2)\n",
    "    quantized_size = quantized_search_results[\"size\"]\n",
    "    improvement_ms = f\"({time_imp}% improvement)\"\n",
    "    improvement_size = f\"({round((full_search_results['size'] - quantized_size) * 100 / full_search_results['size'], 2)}% improvement)\"\n",
    "    print(f\"{index_name} search took: {time_taken:.3f} ms {improvement_ms}\")\n",
    "    print(f\"{index_name} index size: {round(quantized_size / 1000000, 2)} MB {improvement_size}\")\n",
    "    recall = calculate_recall(full_search_results[\"results\"], quantized_search_results[\"results\"])\n",
    "    print(f\"{start_message}: {str(round(recall, 4))}\")\n",
    "\n",
    "def calculate_recall(scored_full_results, scored_quantized_results):\n",
    "    recalls = []\n",
    "    for i in range(len(scored_full_results)):\n",
    "        full_ids = [r[\"id\"] for r in scored_full_results[i]]\n",
    "        quantized_ids = [r[\"id\"] for r in scored_quantized_results[i]]\n",
    "        recalls.append((len(set(full_ids).intersection(set(quantized_ids))) /\n",
    "                       len(set(quantized_ids))))\n",
    "    return sum(recalls) / len(recalls)\n",
    "\n",
    "def generate_search_results(faiss_scores, faiss_ids):\n",
    "    outdoors_data = get_outdoors_data()\n",
    "    faiss_results = []\n",
    "    for i in range(len(faiss_scores)):\n",
    "        results = []\n",
    "        for j, id in enumerate(faiss_ids[i]):\n",
    "            id = int(id)\n",
    "            result = {\"score\": faiss_scores[i][j],\n",
    "                      \"title\": outdoors_data[id][\"title\"],\n",
    "                      \"body\": outdoors_data[id][\"body\"],\n",
    "                      \"id\": id}\n",
    "            results.append(result)\n",
    "        faiss_results.append(results)\n",
    "    return faiss_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 13.20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing full-precision embeddings using FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers.quantization import quantize_embeddings\n",
    "\n",
    "model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\",\n",
    "                            similarity_fn_name=SimilarityFunction.DOT_PRODUCT,\n",
    "                            truncate_dim=1024)\n",
    "\n",
    "def index_full_precision_embeddings(doc_embeddings, name):\n",
    "    index = faiss.IndexFlatIP(doc_embeddings.shape[1])\n",
    "    index.add(doc_embeddings)\n",
    "    faiss.write_index(index, name)\n",
    "    return index\n",
    "\n",
    "def get_outdoors_embeddings(model):\n",
    "    outdoors_dataframe = load_dataframe(\"data/outdoors/posts.csv\")\n",
    "    post_texts = [post[\"title\"] + \" \" + post[\"body\"]\n",
    "                  for post in outdoors_dataframe.collect()]\n",
    "    return numpy.array(get_embeddings(post_texts, model, \"outdoors_mrl_normed\")) #TODO: This will take 2-5 hours to run the first time if not cached. Upload to github to save readers hassle.\n",
    "\n",
    "\n",
    "doc_embeddings = get_outdoors_embeddings(model) #takes 2-3 hours if not cached\n",
    "full_index = index_full_precision_embeddings(doc_embeddings, \"full_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 13.21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating full-precision query embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tent poles', 'hiking trails', 'mountain forests', 'white water', 'best waterfalls', 'mountain biking', 'snowboarding slopes', 'bungee jumping', 'public parks']\n"
     ]
    }
   ],
   "source": [
    "def format_query_for_encoding(query):\n",
    "    #return \"Represent this sentence for searching relevant passages: \" + query\n",
    "    return query\n",
    "    \n",
    "def get_test_queries():\n",
    "    queries = [\"tent poles\", \"hiking trails\", \"mountain forests\",\n",
    "            \"white water\", \"best waterfalls\", \"mountain biking\",\n",
    "            \"snowboarding slopes\", \"bungee jumping\", \"public parks\"]\n",
    "    return list(map(format_query_for_encoding, queries))\n",
    "\n",
    "queries = get_test_queries()\n",
    "print(queries)\n",
    "query_embeddings = model.encode(queries, convert_to_numpy=True,\n",
    "                                normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Listing 13.22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Functions to benchmark quantized search approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_and_execute_search(index, index_name, query_embeddings, k=25):\n",
    "    start_time = time.time()\n",
    "    faiss_scores, faiss_ids = index.search(query_embeddings, k=k)\n",
    "    time_taken = ((time.time() - start_time) * 1000)\n",
    "    \n",
    "    results = {\"results\": generate_search_results(faiss_scores, faiss_ids),\n",
    "               \"time_taken\": time_taken, \n",
    "               \"faiss_scores\": faiss_scores, \"faiss_ids\": faiss_ids}\n",
    "    index_stats = {}\n",
    "    if index_name:\n",
    "        index_stats ={\"index_name\": index_name,\n",
    "                      \"size\": os.path.getsize(index_name)}\n",
    "    return results | index_stats\n",
    "\n",
    "\n",
    "def evaluate_search(full_index, quantized_index, quantized_index_name, query_embeddings, quantized_query_embeddings,\n",
    "                    k=25, display=True, log=False):\n",
    "    full_results = time_and_execute_search(full_index, \"full_embeddings\", query_embeddings, k=k)\n",
    "    quantized_results = time_and_execute_search(quantized_index, quantized_index_name,\n",
    "                                                quantized_query_embeddings, k=k)\n",
    "    if display:\n",
    "        display_statistics(full_results, quantized_results)\n",
    "    return quantized_results, full_results\n",
    "\n",
    "def evaluate_rerank_search(full_index, quantized_index, query_embeddings,\n",
    "                           quantized_embeddings, k=50, limit=25):\n",
    "    results, full_results = evaluate_search(full_index, quantized_index, None, query_embeddings,\n",
    "                                            quantized_embeddings, display=False, k=k)\n",
    "    \n",
    "    doc_embeddings = get_outdoors_embeddings(model) #This can point to a cheap on-disk data source containing the original full-precision embeddings\n",
    "    rescore_scores, rescore_ids = [], []\n",
    "    for i in range(len(results[\"results\"])):\n",
    "        embedding_ids = results[\"faiss_ids\"][i]\n",
    "        top_k_embeddings = [doc_embeddings[id] for id in embedding_ids]\n",
    "        query_embedding = query_embeddings[i]\n",
    "        scores = query_embedding @ numpy.array(top_k_embeddings).T\n",
    "        indices = scores.argsort()[::-1][:limit]\n",
    "        top_k_indices = embedding_ids[indices]\n",
    "        top_k_scores = scores[indices]\n",
    "        rescore_scores.append(top_k_scores)\n",
    "        rescore_ids.append(top_k_indices)\n",
    "\n",
    "    results = generate_search_results(rescore_scores, rescore_ids)\n",
    "    recall = calculate_recall(full_results[\"results\"], results)\n",
    "    print(f\"Reranked recall: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 13.23\n",
    "### int8 quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int8 embeddings shape: (18456, 1024)\n",
      "int8_embeddings search took: 69.244 ms (-949.73% improvement)\n",
      "int8_embeddings index size: 18.91 MB (74.99% improvement)\n",
      "Recall: 0.9289\n",
      "Reranked recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "def index_int8_embeddings(doc_embeddings, name):\n",
    "    int8_embeddings = quantize_embeddings(doc_embeddings, precision=\"int8\")\n",
    "    print(\"Int8 embeddings shape:\", int8_embeddings.shape)\n",
    "    index = faiss.IndexScalarQuantizer(int8_embeddings.shape[1],\n",
    "                                       faiss.ScalarQuantizer.QT_8bit, )\n",
    "    index.train(int8_embeddings)\n",
    "    index.add(int8_embeddings)\n",
    "    faiss.write_index(index, name)\n",
    "    return index\n",
    "\n",
    "int8_index_name = \"int8_embeddings\"\n",
    "int8_index = index_int8_embeddings(doc_embeddings, int8_index_name)\n",
    "\n",
    "quantized_queries = quantize_embeddings(query_embeddings,\n",
    "                                        calibration_embeddings=doc_embeddings,\n",
    "                                        precision=\"int8\")\n",
    "\n",
    "evaluate_search(full_index, int8_index, int8_index_name, query_embeddings, quantized_queries)\n",
    "evaluate_rerank_search(full_index, int8_index, query_embeddings, quantized_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 13.24\n",
    "### Binary Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary embeddings shape: (18456, 128)\n",
      "binary_embeddings search took: 5.629 ms (96.27% improvement)\n",
      "binary_embeddings index size: 2.36 MB (96.87% improvement)\n",
      "Recall: 0.6044\n",
      "Reranked recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "def index_binary_embeddings(doc_embeddings, binary_index_name):\n",
    "    binary_embeddings = quantize_embeddings(doc_embeddings,\n",
    "                                    precision=\"binary\").astype(numpy.uint8)\n",
    "    print(\"Binary embeddings shape:\", binary_embeddings.shape)\n",
    "    index = faiss.IndexBinaryFlat(binary_embeddings.shape[1] * 8)\n",
    "    index.add(binary_embeddings)\n",
    "    faiss.write_index_binary(index, binary_index_name)\n",
    "    return index\n",
    "\n",
    "binary_index_name = \"binary_embeddings\"\n",
    "binary_index = index_binary_embeddings(doc_embeddings, binary_index_name)\n",
    "\n",
    "quantized_queries = quantize_embeddings(query_embeddings,\n",
    "                        calibration_embeddings=doc_embeddings,\n",
    "                                       precision=\"binary\").astype(numpy.uint8)\n",
    "\n",
    "evaluate_search(full_index, binary_index, binary_index_name, query_embeddings, quantized_queries)\n",
    "evaluate_rerank_search(full_index, binary_index, query_embeddings, quantized_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 13.25\n",
    "### Product Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pq_embeddings search took: 12.890 ms (92.49% improvement)\n",
      "pq_embeddings index size: 2.23 MB (97.05% improvement)\n",
      "Recall: 0.5778\n",
      "Reranked recall: 0.991111111111111\n"
     ]
    }
   ],
   "source": [
    "def index_pq_embeddings(doc_embeddings, index_name):\n",
    "    dimensions = doc_embeddings.shape[1]\n",
    "    M_subvectors = 64\n",
    "    num_bits = 8\n",
    "    index = faiss.IndexPQ(dimensions, M_subvectors, num_bits)\n",
    "    index.train(doc_embeddings)\n",
    "    index.add(doc_embeddings)   \n",
    "    faiss.write_index(index, index_name) # Commit the index to disk\n",
    "    return index\n",
    "\n",
    "pq_index_name = \"pq_embeddings\"\n",
    "pq_index = index_pq_embeddings(doc_embeddings, pq_index_name)\n",
    "\n",
    "evaluate_search(full_index, pq_index, pq_index_name, query_embeddings, query_embeddings)\n",
    "evaluate_rerank_search(full_index, pq_index, query_embeddings, query_embeddings)\n",
    "\n",
    "#TODO: consider adding IVFFlatPQ as optimization for speed to show at end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 13.26\n",
    "### Matryoshka Representations Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original embeddings shape: (18456, 1024)\n",
      "\n",
      "\n",
      "mrl_embeddings_512 embeddings shape: (18456, 512)\n",
      "mrl_embeddings_512 search took: 4.079 ms (77.53% improvement)\n",
      "mrl_embeddings_512 index size: 37.8 MB (50.0% improvement)\n",
      "Recall: 0.7022\n",
      "Reranked recall: 1.0\n",
      "\n",
      "\n",
      "mrl_embeddings_256 embeddings shape: (18456, 256)\n",
      "mrl_embeddings_256 search took: 2.212 ms (74.8% improvement)\n",
      "mrl_embeddings_256 index size: 18.9 MB (75.0% improvement)\n",
      "Recall: 0.4756\n",
      "Reranked recall: 0.9688888888888888\n",
      "\n",
      "\n",
      "mrl_embeddings_128 embeddings shape: (18456, 128)\n",
      "mrl_embeddings_128 search took: 1.592 ms (81.34% improvement)\n",
      "mrl_embeddings_128 index size: 9.45 MB (87.5% improvement)\n",
      "Recall: 0.2489\n",
      "Reranked recall: 0.64\n"
     ]
    }
   ],
   "source": [
    "def get_mrl_embeddings(embeddings, num_dimensions):\n",
    "    mrl_embeddings = numpy.array(list(map(lambda e: e[:num_dimensions], embeddings)))\n",
    "    return mrl_embeddings\n",
    "\n",
    "def index_mrl_embeddings(doc_embeddings, num_dimensions, mrl_index_name):\n",
    "    mrl_doc_embeddings = get_mrl_embeddings(doc_embeddings, num_dimensions)\n",
    "    print(f\"{mrl_index_name} embeddings shape:\", mrl_doc_embeddings.shape)\n",
    "    mrl_index = index_full_precision_embeddings(mrl_doc_embeddings, mrl_index_name)\n",
    "    return mrl_index\n",
    "\n",
    "print(f\"Original embeddings shape:\", doc_embeddings.shape)\n",
    "original_dimensions = doc_embeddings.shape[1] #1024\n",
    "\n",
    "for num_dimensions in [original_dimensions//2, #512\n",
    "                       original_dimensions//4, #256\n",
    "                       original_dimensions//8]: #128\n",
    "    print(\"\\n\")\n",
    "    mrl_index_name = f\"mrl_embeddings_{num_dimensions}\"\n",
    "    mrl_index = index_mrl_embeddings(doc_embeddings, num_dimensions, mrl_index_name)\n",
    "    mrl_queries = get_mrl_embeddings(query_embeddings, num_dimensions)\n",
    "\n",
    "    evaluate_search(full_index, mrl_index, mrl_index_name, query_embeddings, mrl_queries)\n",
    "    evaluate_rerank_search(full_index, mrl_index, query_embeddings, mrl_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 13.27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_ivf_halfmrl_embeddings search took: 0.807 ms (91.78% improvement)\n",
      "binary_ivf_halfmrl_embeddings index size: 1.35 MB (98.22% improvement)\n",
      "Recall: 0.3511\n",
      "Reranked recall: 0.7244444444444444\n"
     ]
    }
   ],
   "source": [
    "def get_mrl_embeddings(embeddings, num_dimensions):\n",
    "    mrl_embeddings = numpy.array(list(map(lambda e: e[:num_dimensions], embeddings)))\n",
    "    return mrl_embeddings\n",
    "\n",
    "def index_binary_ivf_halffmrl_embeddings(doc_embeddings, num_dimensions, binary_index_name):\n",
    "\n",
    "    #MRL\n",
    "    half_mrl_doc_embeddings =  get_mrl_embeddings(doc_embeddings, num_dimensions)\n",
    "    \n",
    "    #Binary quantization\n",
    "    binary_embeddings = quantize_embeddings(half_mrl_doc_embeddings,\n",
    "                                    calibration_embeddings=half_mrl_doc_embeddings,\n",
    "                                    precision=\"binary\").astype(numpy.uint8)\n",
    "    d = half_mrl_doc_embeddings.shape[1]\n",
    "    quantizer = faiss.IndexBinaryFlat(d)\n",
    "    \n",
    "    #ANN: IVF Flat Algorithm\n",
    "    index = faiss.IndexBinaryIVF(quantizer, d, nlist)\n",
    "    index.nprobe = 4\n",
    "\n",
    "    index.train(binary_embeddings)\n",
    "    index.add(binary_embeddings)\n",
    "    faiss.write_index_binary(index, binary_index_name)\n",
    "    return index\n",
    "\n",
    "mrl_dimensions = doc_embeddings.shape[1] // 2  #MRL 1024 => 512 dimensions\n",
    "binary_ivf_halffmrl_index_name = \"binary_ivf_halfmrl_embeddings\"\n",
    "binary_ivf_halffmrl_index = index_binary_ivf_halffmrl_embeddings(doc_embeddings, mrl_dimensions, binary_ivf_halffmrl_index_name)\n",
    "\n",
    "half_mrl_doc_embeddings =  get_mrl_embeddings(doc_embeddings, mrl_dimensions)\n",
    "halfmrl_queries = get_mrl_embeddings(query_embeddings, mrl_dimensions)\n",
    "quantized_queries = quantize_embeddings(halfmrl_queries,\n",
    "                        calibration_embeddings=half_mrl_doc_embeddings,\n",
    "                                       precision=\"binary\").astype(numpy.uint8)\n",
    "\n",
    "evaluate_search(full_index, binary_ivf_halffmrl_index, binary_ivf_halffmrl_index_name, query_embeddings, quantized_queries)\n",
    "evaluate_rerank_search(full_index, binary_ivf_halffmrl_index, query_embeddings, quantized_queries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
