{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally:\n",
    "1. We use outdoors dataset for all examples w/ https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1 model\n",
    "2. We have a general \"test_recall\" method that takes in an original ranked list or results plus any number of other lists and compares recall of the top-N\n",
    "3. We have the following code examples, each of which output the recall for each set of results passed in. Possibly also the first few results where it makes sense to show the output.\n",
    "\n",
    "----1. Scalar quantization (Int8 , Int4) **NOTE: can leave Int4 off if unsupported by SentenceTransformers (I think this may be the case)**\n",
    "\n",
    "--------- SentenceTransformers as library\n",
    "\n",
    "--------- code using it tests original vs. Int 8 vs. Int8 w/rescoring\n",
    "\n",
    "--------- second listing tests original vs. Int4 vs. Int4 w/rescoring\n",
    "\n",
    "----2. Binary quantization\n",
    "\n",
    "--------- Sentence transformers as library\n",
    "\n",
    "--------- code using it tests original vs. bq vs. bq w/ rescoring\n",
    "\n",
    "----3. Matroyoshka Learned Representations\n",
    "\n",
    "-------- Sentence transformers as library\n",
    "\n",
    "--------- code using it tests original vs. MRL @ 1/2 vs MRL @ 1/4 vs. MRL @ 1/8 vs. MRL @ 1/2 w/rescoring vs. MRL @ 1/4 w/rescoring vs. MRL @ 1/8 w/rescoring\n",
    "\n",
    "----3. Product quantization\n",
    "\n",
    "-------- nanopq as library (I don't think sentence tranformers does product quantization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Daniel's Original Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from aips import get_engine\n",
    "from pyspark.sql import SparkSession\n",
    "import pickle \n",
    "import numpy \n",
    "import torch\n",
    "import clip\n",
    "import time\n",
    "import pandas\n",
    "import random\n",
    "import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "engine = get_engine()\n",
    "spark = SparkSession.builder.appName(\"AIPS\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n",
      "movies_with_image_embeddings.pickle\n"
     ]
    }
   ],
   "source": [
    "![ ! -d 'tmdb' ] && git clone --depth 1 https://github.com/ai-powered-search/tmdb.git\n",
    "! cd tmdb && git pull\n",
    "! cd tmdb && mkdir -p '../data/tmdb/' && tar -xvf movies_with_image_embeddings.tgz -C '../data/tmdb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embedding(embedding):\n",
    "    return numpy.divide(embedding,\n",
    "      numpy.linalg.norm(embedding,axis=0)).tolist()\n",
    "\n",
    "def read(cache_name):\n",
    "    cache_file_name = f\"data/tmdb/{cache_name}.pickle\"\n",
    "    with open(cache_file_name, \"rb\") as fd:\n",
    "        return pickle.load(fd)\n",
    "\n",
    "def quantize(embeddings):\n",
    "    embeddings = numpy.array(embeddings)\n",
    "    quantized_embeddings = numpy.zeros_like(embeddings, dtype=numpy.int8)\n",
    "    quantized_embeddings[embeddings > 0] = 1\n",
    "    return quantized_embeddings.tolist()\n",
    "\n",
    "def tmdb_with_embeddings_dataframe():\n",
    "    movies = read(\"movies_with_image_embeddings\")\n",
    "    embeddings = movies[\"image_embeddings\"]\n",
    "    normalized_embeddings = [normalize_embedding(e) for e in embeddings]\n",
    "    quantized_embeddings = [quantize(e) for e in normalized_embeddings]\n",
    "    movie_dataframe = spark.createDataFrame(\n",
    "        zip(movies[\"movie_ids\"], movies[\"titles\"], \n",
    "            movies[\"image_ids\"], normalized_embeddings,\n",
    "            quantized_embeddings),\n",
    "        schema=[\"movie_id\", \"title\", \"image_id\", \"image_embedding\",\n",
    "                \"image_binary_embedding\"])\n",
    "    return movie_dataframe\n",
    "    \n",
    "def encode_text(text):\n",
    "    text = clip.tokenize([text]).to(device)\n",
    "    text_features = model.encode_text(text).tolist()[0]\n",
    "    return numpy.array(normalize_embedding(text_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiping \"tmdb_with_embeddings\" collection\n",
      "Creating \"tmdb_with_embeddings\" collection\n",
      "Status: Success\n",
      "Successfully written 7549 documents\n"
     ]
    }
   ],
   "source": [
    "movie_dataframe = tmdb_with_embeddings_dataframe()\n",
    "embeddings_collection = engine.create_collection(\"tmdb_with_embeddings\")\n",
    "embeddings_collection.write(movie_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_list(dataframe, column):\n",
    "    return numpy.array(dataframe.select(column).rdd.flatMap(lambda x: x).collect())\n",
    "\n",
    "def sort_titles(scores, movies, limit=25):\n",
    "    titles = column_list(movies, \"title\").tolist()\n",
    "    binary_results = numpy.argsort(scores)[-limit:][::-1]\n",
    "    ranked = [titles[id] for id in binary_results]\n",
    "    return list(dict.fromkeys(ranked))\n",
    "\n",
    "def numpy_rankings(query, limit=20):\n",
    "    start_dotprod = time.time()\n",
    "\n",
    "    embeddings = column_list(movie_dataframe, \"image_embedding\")\n",
    "    query_embedding = encode_text(query)\n",
    "    dot_prod_scores = numpy.dot(embeddings, query_embedding)\n",
    "\n",
    "    stop_dotprod = time.time(); start_binary = time.time()\n",
    "\n",
    "    quantized_embeddings = column_list(movie_dataframe, \"image_binary_embedding\")\n",
    "    quantized_query = numpy.array(quantize(query_embedding)) \n",
    "    binary_scores = 1536 - numpy.logical_xor(quantized_embeddings,\n",
    "                                             quantized_query).sum(axis=1)\n",
    "    \n",
    "    stop_binary = time.time()\n",
    "    \n",
    "    binary_results = sort_titles(binary_scores, movie_dataframe)\n",
    "    full_results = sort_titles(dot_prod_scores, movie_dataframe)\n",
    "    return {\"binary_query_time\": stop_binary - start_binary,\n",
    "            \"full_query_time\": stop_dotprod - start_dotprod,\n",
    "            \"recall\": len(set(full_results).intersection(set(binary_results))) / len(set(binary_results)),\n",
    "            \"binary_results\": binary_results,\n",
    "            \"full_results\": full_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_titles(response):\n",
    "    return [d[\"title\"] for d in response[\"docs\"]]\n",
    "\n",
    "def base_search_request(query_vector, field, quantization_size):\n",
    "    return {\"query\": query_vector,\n",
    "            \"query_fields\": [field],\n",
    "            \"return_fields\": [\"movie_id\", \"title\", \"score\"],\n",
    "            \"limit\": 25,\n",
    "            \"k\": 1000,\n",
    "            \"quantization_size\": quantization_size}\n",
    "\n",
    "def engine_rankings(query, log=False):\n",
    "    collection = engine.get_collection(\"tmdb_with_embeddings\")\n",
    "    query_embedding = encode_text(query)    \n",
    "    quantized_query = numpy.zeros_like(query_embedding, dtype=numpy.int8)\n",
    "    quantized_query[query_embedding > 0] = 1\n",
    "\n",
    "    binary_request = base_search_request(quantized_query.tolist(),\n",
    "                                         \"image_binary_embedding\",\n",
    "                                         \"BINARY\")\n",
    "    start_dotprod = time.time()    \n",
    "    binary_results = only_titles(collection.search(**binary_request))\n",
    "    stop_dotprod = time.time()\n",
    "\n",
    "    reranked_request = binary_request\n",
    "    reranked_request[\"rerank_query\"] = {\n",
    "        \"query\": query_embedding.tolist(),\n",
    "        \"query_fields\": [\"image_embedding\"],\n",
    "        \"k\": 100,\n",
    "        \"rerank_count\": 100,\n",
    "        \"quantization_size\": \"FLOAT32\"}\n",
    "    \n",
    "    if log: print(json.dumps(reranked_request, indent=2))        \n",
    "    start_reranked = time.time()    \n",
    "    full_results = only_titles(collection.search(**reranked_request))\n",
    "    stop_reranked = time.time()\n",
    "    return {\"binary_query_time\": stop_reranked - start_reranked,\n",
    "            \"full_query_time\": stop_dotprod - start_dotprod,\n",
    "            \"recall\": len(set(full_results).intersection(set(binary_results))) / len(set(binary_results)),\n",
    "            \"binary_results\": binary_results,\n",
    "            \"full_results\": full_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search engine binary search time: 0.030432939529418945\n",
      "Search engine full search time: 0.0288541316986084\n",
      "Numpy binary search time: 1.6173162460327148\n",
      "Numpy full search time: 2.3195929527282715\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quantized solr</th>\n",
       "      <th>quantized numpy</th>\n",
       "      <th>dotprod solr</th>\n",
       "      <th>dotprod numpy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Hobbit: The Desolation of Smaug</td>\n",
       "      <td>The Hobbit: The Desolation of Smaug</td>\n",
       "      <td>The Lord of the Rings: The Fellowship of the Ring</td>\n",
       "      <td>The Lord of the Rings: The Fellowship of the Ring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Lord of the Rings: The Fellowship of the Ring</td>\n",
       "      <td>The Lord of the Rings: The Fellowship of the Ring</td>\n",
       "      <td>The Hobbit: An Unexpected Journey</td>\n",
       "      <td>The Hobbit: An Unexpected Journey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Klaus</td>\n",
       "      <td>The Hobbit: The Battle of the Five Armies</td>\n",
       "      <td>The Princess Bride</td>\n",
       "      <td>The Princess Bride</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Hobbit: The Desolation of Smaug</td>\n",
       "      <td>Klaus</td>\n",
       "      <td>The Hobbit: The Battle of the Five Armies</td>\n",
       "      <td>The Hobbit: The Battle of the Five Armies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Hobbit: The Battle of the Five Armies</td>\n",
       "      <td>The Goonies</td>\n",
       "      <td>The Hobbit: The Battle of the Five Armies</td>\n",
       "      <td>The Hobbit: The Desolation of Smaug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Goonies</td>\n",
       "      <td>The Hobbit: An Unexpected Journey</td>\n",
       "      <td>The Hobbit: An Unexpected Journey</td>\n",
       "      <td>The Lord of the Rings: The Two Towers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Hobbit: The Battle of the Five Armies</td>\n",
       "      <td>Labyrinth</td>\n",
       "      <td>The Lord of the Rings: The Fellowship of the Ring</td>\n",
       "      <td>The Lord of the Rings: The Return of the King</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Labyrinth</td>\n",
       "      <td>The Lord of the Rings: The Return of the King</td>\n",
       "      <td>The Hobbit: The Desolation of Smaug</td>\n",
       "      <td>Guardians of the Galaxy Vol. 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Hobbit: The Desolation of Smaug</td>\n",
       "      <td>Frozen II</td>\n",
       "      <td>The Lord of the Rings: The Fellowship of the Ring</td>\n",
       "      <td>The Last Samurai</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      quantized solr  \\\n",
       "0                The Hobbit: The Desolation of Smaug   \n",
       "1  The Lord of the Rings: The Fellowship of the Ring   \n",
       "2                                              Klaus   \n",
       "3                The Hobbit: The Desolation of Smaug   \n",
       "4          The Hobbit: The Battle of the Five Armies   \n",
       "5                                        The Goonies   \n",
       "6          The Hobbit: The Battle of the Five Armies   \n",
       "7                                          Labyrinth   \n",
       "8                The Hobbit: The Desolation of Smaug   \n",
       "\n",
       "                                     quantized numpy  \\\n",
       "0                The Hobbit: The Desolation of Smaug   \n",
       "1  The Lord of the Rings: The Fellowship of the Ring   \n",
       "2          The Hobbit: The Battle of the Five Armies   \n",
       "3                                              Klaus   \n",
       "4                                        The Goonies   \n",
       "5                  The Hobbit: An Unexpected Journey   \n",
       "6                                          Labyrinth   \n",
       "7      The Lord of the Rings: The Return of the King   \n",
       "8                                          Frozen II   \n",
       "\n",
       "                                        dotprod solr  \\\n",
       "0  The Lord of the Rings: The Fellowship of the Ring   \n",
       "1                  The Hobbit: An Unexpected Journey   \n",
       "2                                 The Princess Bride   \n",
       "3          The Hobbit: The Battle of the Five Armies   \n",
       "4          The Hobbit: The Battle of the Five Armies   \n",
       "5                  The Hobbit: An Unexpected Journey   \n",
       "6  The Lord of the Rings: The Fellowship of the Ring   \n",
       "7                The Hobbit: The Desolation of Smaug   \n",
       "8  The Lord of the Rings: The Fellowship of the Ring   \n",
       "\n",
       "                                       dotprod numpy  \n",
       "0  The Lord of the Rings: The Fellowship of the Ring  \n",
       "1                  The Hobbit: An Unexpected Journey  \n",
       "2                                 The Princess Bride  \n",
       "3          The Hobbit: The Battle of the Five Armies  \n",
       "4                The Hobbit: The Desolation of Smaug  \n",
       "5              The Lord of the Rings: The Two Towers  \n",
       "6      The Lord of the Rings: The Return of the King  \n",
       "7                     Guardians of the Galaxy Vol. 2  \n",
       "8                                   The Last Samurai  "
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"The Hobbit\"\n",
    "engine_scores = engine_rankings(query)\n",
    "numpy_scores = numpy_rankings(query)\n",
    "results = pandas.DataFrame(zip(engine_scores[\"binary_results\"], numpy_scores[\"binary_results\"],\n",
    "                          engine_scores[\"full_results\"], numpy_scores[\"full_results\"]),\n",
    "                          columns=[\"quantized solr\", \"quantized numpy\",\n",
    "                                   \"dotprod solr\", \"dotprod numpy\"])\n",
    "print(f\"Search engine binary search time: {engine_scores['binary_query_time']}\")\n",
    "print(f\"Search engine full search time: {engine_scores['full_query_time']}\")\n",
    "print(f\"Numpy binary search time: {numpy_scores['binary_query_time']}\")\n",
    "print(f\"Numpy full search time: {numpy_scores['full_query_time']}\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [02:21<00:00,  5.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average quantized recall for numpy: 0.3250562238049114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  5.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average quantized recall for engine: 0.5726311496493216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "\n",
    "titles = column_list(movie_dataframe, \"title\")\n",
    "random.shuffle(titles)\n",
    "\n",
    "def mean_accuracy(f):\n",
    "    return numpy.mean([f(q)[\"recall\"] for q in tqdm.tqdm(titles[:25])])\n",
    "\n",
    "print(f\"Average quantized recall for numpy: {mean_accuracy(numpy_rankings)}\")\n",
    "print(f\"Average quantized recall for engine: {mean_accuracy(engine_rankings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_embeddings = numpy.array(column_list(movie_dataframe, \"image_embedding\")[0:1000]) #100 movie image embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the mxabi-embed-large-v1 model with SentenceTransfomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SentenceTransformer.__init__() got an unexpected keyword argument 'truncate_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[222], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m matryoshka_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#model = SentenceTransformer(\"tomaarsen/mpnet-base-nli-matryoshka\", truncate_dim=matryoshka_dim)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmixedbread-ai/mxbai-embed-large-v1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatryoshka_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(\n\u001b[1;32m      9\u001b[0m     [\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe weather is so nice!\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     ]\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(embeddings\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mTypeError\u001b[0m: SentenceTransformer.__init__() got an unexpected keyword argument 'truncate_dim'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "matryoshka_dim = 64\n",
    "#model = SentenceTransformer(\"tomaarsen/mpnet-base-nli-matryoshka\", truncate_dim=matryoshka_dim)\n",
    "model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\", truncate_dim=matryoshka_dim)\n",
    "\n",
    "embeddings = model.encode(\n",
    "    [\n",
    "        \"The weather is so nice!\",\n",
    "        \"It's so sunny outside!\",\n",
    "        \"He drove to the stadium.\",\n",
    "    ]\n",
    ")\n",
    "print(embeddings.shape)\n",
    "# => (3, 64)\n",
    "\n",
    "# Similarity of the first sentence to the other two:\n",
    "similarities = cos_sim(embeddings[0], embeddings[1:])\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scalar Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO: use SentenceTransformers instead of showing the internal code\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.quantization import quantize_embeddings\n",
    "\n",
    "int8_embeddings = quantize_embeddings(embeddings, precision=\"int8\") #float32”, “int8”, “uint8”, “binary”, “ubinary”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Binary Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO: use SentenceTransformers instead of showing the internal code\n",
    "binary_embeddings = quantize_embeddings(embeddings, precision=\"binary\") # Note: \"binary\" is np.packedbits and \"ubinary\" maps into uInt8. Note sure what we need here, but be aware we have both types available. #See: https://sbert.net/examples/applications/embedding-quantization/README.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matroyoshka Learned Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO: just divide the embedding by 1/2, 1/4, 1/8. Should'nt need anything fancy other than embeddings trained on the MRL-compatible model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Product quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/var/cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -uggingface-hub (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: nanopq in /opt/conda/lib/python3.10/site-packages (0.2.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from nanopq) (1.23.5)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from nanopq) (1.10.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution - (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -uggingface-hub (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install nanopq \n",
    "#move to requirements.txt w/ version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "M: 8, Ks: 256, metric : <class 'numpy.uint8'>, code_dtype: l2\n",
      "iter: 20, seed: 123\n",
      "Training the subspace: 0 / 8\n",
      "Training the subspace: 1 / 8\n",
      "Training the subspace: 2 / 8\n",
      "Training the subspace: 3 / 8\n",
      "Training the subspace: 4 / 8\n",
      "Training the subspace: 5 / 8\n",
      "Training the subspace: 6 / 8\n",
      "Training the subspace: 7 / 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<nanopq.pq.PQ at 0x7f648d4543d0>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nanopq\n",
    "import numpy as np\n",
    "\n",
    "embeddings = numpy.array(original_embeddings, dtype=np.float32) #convert to float32 from float64. May not be needed depending on if they're already in that format.\n",
    "\n",
    "N = embeddings.shape[0] #documents\n",
    "D = embeddings.shape[1] #dimensions/features\n",
    "M=8 #number of subvectors \n",
    "print(embeddings.dtype)\n",
    "\n",
    "# Instantiate with M=8 sub-spaces\n",
    "pq = nanopq.PQ(M=M)\n",
    "\n",
    "# Train codewords\n",
    "pq.fit(embeddings)  #NOTE: this can be trained on a training set or a subset of the embeddings if this is too slow. It's doing Kmeans to generate clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding the subspace: 0 / 8\n",
      "Encoding the subspace: 1 / 8\n",
      "Encoding the subspace: 2 / 8\n",
      "Encoding the subspace: 3 / 8\n",
      "Encoding the subspace: 4 / 8\n",
      "Encoding the subspace: 5 / 8\n",
      "Encoding the subspace: 6 / 8\n",
      "Encoding the subspace: 7 / 8\n"
     ]
    }
   ],
   "source": [
    "# \"Index documents\": Encode to PQ-codes\n",
    "quantized_documents = pq.encode(embeddings)  # with dtype=np.uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[235], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m quantized_query \u001b[38;5;241m=\u001b[39m \u001b[43mpq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#grab any document that makes for a good example, but let's use the same example throughout all quantization types for consistency\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Results: create a distance table online, and compute Asymmetric Distance to each PQ-code \u001b[39;00m\n\u001b[1;32m      4\u001b[0m dists \u001b[38;5;241m=\u001b[39m pq\u001b[38;5;241m.\u001b[39mdtable(quantized_query)\u001b[38;5;241m.\u001b[39madist(quantized_documents)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nanopq/pq.py:137\u001b[0m, in \u001b[0;36mPQ.encode\u001b[0;34m(self, vecs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Encode input vectors into PQ-codes.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m \n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m vecs\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m vecs\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    138\u001b[0m N, D \u001b[38;5;241m=\u001b[39m vecs\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m D \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDs \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mM, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput dimension must be Ds * M\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "quantized_query = pq.encode(embeddings[0]) #grab any document that makes for a good example, but let's use the same example throughout all quantization types for consistency\n",
    "\n",
    "# Results: create a distance table online, and compute Asymmetric Distance to each PQ-code \n",
    "dists = pq.dtable(quantized_query).adist(quantized_documents)\n",
    "\n",
    "#TODO: sort results by dists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGNORE BELOW: Throw-away code / not necessarily functional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scalar Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform Scalar Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_embeddings = numpy.array(column_list(movie_dataframe, \"image_embedding\")[0:1000]) #100 movie image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_comparison(original_embeddings, quantized_embeddings, dequantized_embeddings):\n",
    "    print(\"Original:\", f\"[ {', '.join([str(emb) for emb in original_embeddings[0][0:4]]) + ' ... ' + ', '.join([str(emb) for emb in original_embeddings[0][-4:]])}]\", \"Memory Usage: \", sys.getsizeof(original_embeddings[0]))\n",
    "    print(\"Quantized:\", f\"[ {', '.join([str(emb) for emb in quantized_embeddings[0][0:4]]) + ' ... ' + ', '.join([str(emb) for emb in quantized_embeddings[0][-4:]])}]\", \"Memory Usage: \", sys.getsizeof(quantized_embeddings[0]))\n",
    "    print(\"Dequantized:\", f\"[ {', '.join([str(emb) for emb in dequantized_embeddings[0][0:4]]) + ' ... ' + ', '.join([str(emb) for emb in dequantized_embeddings[0][-4:]])}]\", \"Memory Usage: \", sys.getsizeof(dequantized_embeddings[0]))\n",
    "\n",
    "    #    print(\"Quantized:\", quantized_embeddings[0][0:4, \"Memory Usage: \", sys.getsizeof(quantized[0]))\n",
    "#    print(\"Dequantized:\", dequantized[0])\n",
    "    print(\"\\nSimilarity, Original vs. Dequantized:\", np.cos(original_embeddings[1],dequantized_embeddings[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def uniform_scalar_quantize(embeddings, bits):\n",
    "    # Determine the range of the embeddings\n",
    "    min_val, max_val = embeddings.min(), embeddings.max()\n",
    "    \n",
    "    # Calculate the step size\n",
    "    step = (max_val - min_val) / (2**bits - 1)\n",
    "    \n",
    "    # Quantize the embeddings\n",
    "    quantized = np.round((embeddings - min_val) / step)\n",
    "    \n",
    "    # Clip to ensure values are within the valid range\n",
    "    quantized = np.clip(quantized, 0, 2**bits - 1)\n",
    "    \n",
    "    return quantized.astype(np.uint8 if bits <= 8 else np.uint16)\n",
    "\n",
    "def uniform_scalar_dequantize(quantized, bits, original_min, original_max):\n",
    "    # Calculate the step size\n",
    "    step = (original_max - original_min) / (2**bits - 1)\n",
    "    \n",
    "    # Dequantize the embeddings\n",
    "    dequantized = quantized * step + original_min\n",
    "    \n",
    "    return dequantized\n",
    "\n",
    "# Example usage\n",
    "#original_embeddings = np.random.rand(500, 10)  # 5 embeddings of dimension 10\n",
    "#original_embeddings = column_list(movie_dataframe, \"image_embedding\")\n",
    "\n",
    "#bits = 8  # Quantize to 8 bits\n",
    "\n",
    "#quantized = uniform_scalar_quantize(original_embeddings, bits)\n",
    "#dequantized = uniform_scalar_dequantize(quantized, bits, original_embeddings.min(), original_embeddings.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [ 0.00393834233885761, -0.035241456162213435, -0.035581467904314476, -0.00575613596354904 ... -0.004695540596792066, 0.040802999896525756, 0.0049649511614913875, 0.008843208585659732] Memory Usage:  112\n",
      "Quantized: [ 47159, 44945, 44925, 46611 ... 46671, 49243, 47217, 47436] Memory Usage:  112\n",
      "Dequantized: [ 0.003936196123636382, -0.03523501888379843, -0.03558886906453407, -0.005759298828519999 ... -0.004697748286313197, 0.04080738495628955, 0.004962361647769642, 0.00883702112682494] Memory Usage:  112\n",
      "\n",
      "Similarity, Original vs. Dequantized: [0.99999999 0.99991281 0.99960331 0.99997372 0.99928661 0.9999972\n",
      " 0.9994595  0.99616831 0.99829246 0.99985146 0.99995043 0.99936638\n",
      " 0.99795224 0.99997967 0.99992438 0.99947543 0.99995011 0.99997784\n",
      " 0.99865064 0.99996072 0.99300141 1.         0.9997298  0.999988\n",
      " 0.99993678 0.99969668 0.9992162  0.99984406 0.99996161 0.99928171\n",
      " 0.99998916 0.99904102 0.99870128 0.99965787 0.9999936  0.99992066\n",
      " 0.99980722 0.9973062  0.99999533 0.99474695 0.99999291 0.99999911\n",
      " 0.99984901 0.99977668 0.99999062 0.99758872 0.99992721 0.99980763\n",
      " 0.99964051 0.99995712 0.99983146 0.99991287 0.9980452  0.99981915\n",
      " 0.99980375 0.99984486 0.99967017 0.99994511 0.99988432 0.99938508\n",
      " 0.99987107 0.99888586 0.99995007 0.99997203 0.99954493 0.99934602\n",
      " 0.99945727 0.9999896  0.99996956 0.9999831  0.99907437 0.99951522\n",
      " 0.99956287 0.99815839 0.99995713 0.9992557  0.99978024 0.99999963\n",
      " 0.99929735 0.9998258  0.99999538 0.99952281 0.99999401 0.99939243\n",
      " 0.99999838 0.99904109 0.99999594 0.99910444 0.99997761 0.99970467\n",
      " 0.99991696 0.99891278 0.81328324 0.99964497 0.99998963 0.99999582\n",
      " 0.99976115 0.9999111  0.99962871 0.99738936 0.99989899 0.99953514\n",
      " 0.99943569 0.99988823 0.99967915 0.99860217 0.9965362  0.99970001\n",
      " 0.99995802 0.99971038 0.99983923 0.9991163  0.99962819 0.99991873\n",
      " 0.99999967 0.99989478 0.99894167 0.99798532 0.99943052 0.99999418\n",
      " 0.99982417 0.99823741 0.99999666 0.99879802 0.99886104 0.99936651\n",
      " 0.99995318 0.99999978 0.99996364 0.99983122 0.998927   0.9992958\n",
      " 0.99994535 0.99632688 0.99761674 0.99965865 0.99999192 0.99981058\n",
      " 0.99958252 0.999661   0.99964287 0.99999938 0.99996936 0.99988897\n",
      " 0.99995386 0.99979647 0.99969761 0.99986321 0.99996739 0.99971913\n",
      " 0.99967712 0.99988615 0.99887464 0.99669167 0.99999834 0.99997462\n",
      " 0.99970486 0.99885004 0.99996931 0.99945474 0.99645888 0.99990635\n",
      " 0.99967832 0.99986237 0.99967383 0.99997062 0.99992585 0.99942433\n",
      " 0.99999522 0.99930228 0.99988452 0.99931331 0.99971485 0.99885573\n",
      " 0.99884919 0.99938418 0.99906938 0.99999533 0.99950345 0.99982835\n",
      " 0.99996275 0.99959166 0.9997694  0.99969627 0.99999604 0.99936616\n",
      " 0.99892064 0.99945361 0.99657091 0.9988893  0.99935621 0.99763933\n",
      " 0.99970701 0.9999987  0.9982195  0.99998607 0.99943846 0.99954466\n",
      " 0.99978281 0.99723491 0.99971554 0.99979786 0.99995812 0.99998334\n",
      " 0.99959607 0.99990352 0.99999938 0.99913698 0.99987455 0.99999686\n",
      " 0.99998056 0.99925284 0.99950808 0.99967856 0.99992746 0.99988915\n",
      " 0.99974316 0.99989813 0.99985238 0.99999428 0.99952313 0.99985902\n",
      " 0.99960102 0.99902714 0.9992078  0.99904457 0.99998111 0.99993594\n",
      " 0.9998413  0.99968758 0.99998203 0.99957079 0.99765832 0.99847391\n",
      " 0.99998783 0.99603744 0.99886667 0.99996857 0.99958348 0.99976977\n",
      " 0.99992615 0.99997038 0.99998845 0.99827794 0.99984202 0.99998175\n",
      " 0.99998384 0.99833723 0.99986777 0.99987253 0.99994029 0.99786185\n",
      " 0.99989369 0.99979655 0.99997357 0.99836656 0.99960812 0.99883854\n",
      " 0.98560934 0.99999662 0.99998277 0.99993812 0.99945861 0.99655122\n",
      " 0.99995207 0.99996912 0.99906765 0.99994319 0.99970436 0.99952267\n",
      " 0.99942266 0.99997785 0.99992417 0.99997812 0.99959999 0.9970014\n",
      " 0.99995149 0.99961841 0.99995653 0.99903679 0.99999952 0.99988375\n",
      " 0.99623203 0.99973153 0.99922549 0.99964339 0.99999471 0.99996917\n",
      " 0.99863463 0.99961088 0.99989355 0.99961001 0.99979853 0.9985729\n",
      " 0.99999291 0.99994421 0.99971001 0.99999236 0.99993407 0.99938625\n",
      " 0.99991995 0.99935409 0.99999008 0.99886881 0.9990153  0.99940468\n",
      " 0.99840092 0.99986318 0.99981664 0.99965114 0.99998392 0.99982972\n",
      " 0.99632243 0.99940859 0.99998917 0.99931151 0.99968501 0.99912625\n",
      " 0.9998786  0.99999101 0.99913224 0.99981565 0.99991872 0.99989665\n",
      " 0.99973752 0.99988498 0.99937965 0.99994958 0.99985322 0.99990556\n",
      " 0.99941553 0.99945931 0.99983641 0.99930265 0.99924521 0.9999811\n",
      " 0.99971428 0.99992199 0.99990792 0.99994945 0.99998929 0.99967328\n",
      " 0.99999783 0.99991672 0.99982389 0.99933455 0.9987639  0.99934951\n",
      " 0.99999491 0.99926562 0.99999989 0.99972176 0.99880026 0.99989707\n",
      " 0.99973442 0.99937043 0.99929169 0.99717631 0.99996456 0.99958044\n",
      " 0.99995882 0.99976365 0.99999053 0.99990805 0.99748251 0.99989164\n",
      " 0.99999664 0.99968219 0.99948903 0.99993203 0.99979395 0.99956148\n",
      " 0.99973413 0.9999994  0.99987985 0.99959309 0.99948781 0.99987824\n",
      " 0.9999348  0.99938773 0.99995977 0.99997476 0.99990683 0.99876319\n",
      " 0.99981067 0.99928737 0.99994531 0.99999284 0.99998586 0.99995002\n",
      " 0.99868137 0.99885105 0.99958304 0.99999065 0.99991391 0.99979455\n",
      " 0.99794145 0.99989611 0.99928932 0.99937273 0.99960064 0.99978363\n",
      " 0.98676073 0.99980286 0.9999982  0.99983421 0.99910842 0.99906554\n",
      " 0.99989863 0.99966526 0.99976278 0.9989927  0.99969799 0.99606684\n",
      " 0.99998759 0.99902878 0.99952236 0.99994976 0.99915155 0.99906698\n",
      " 0.99868395 0.99993542 0.99881283 0.99998134 0.99924882 0.99994365\n",
      " 0.99972809 0.99999715 0.99547028 0.99957464 0.99999686 0.99969861\n",
      " 0.99275547 0.99988898 0.99999123 0.99977567 0.99999344 0.99988808\n",
      " 0.9998752  0.99989398 0.99997343 0.99397695 0.99918424 0.99959628\n",
      " 0.99947732 0.99999979 0.99977128 0.99946063 0.9999335  0.99998359\n",
      " 0.99868493 0.99991414 0.99993429 0.99816792 0.99994426 0.99992897\n",
      " 0.99989312 0.99972227 0.99967575 0.99999379 0.99835809 0.99984255\n",
      " 0.99995328 0.99960917 0.99985287 0.99972024 0.99841805 0.99994181\n",
      " 0.9999859  0.99876545 0.99994141 0.99884092 0.99974743 1.\n",
      " 0.99838727 0.99850679 0.99990151 0.99938097 0.99999046 0.99988484\n",
      " 0.99999794 0.99933159 0.99999463 0.99896959 0.99977299 0.99924103\n",
      " 0.99904299 0.99953036 0.99992028 1.         0.99999888 0.99988297\n",
      " 0.99970472 0.99988495 0.99995571 0.99959658 0.9999992  0.9988687\n",
      " 0.99982322 0.9994368  0.99999853 0.99992258 0.99914016 0.998942\n",
      " 0.99756303 0.99999142 0.99995964 0.99998167 0.99936346 0.99959967\n",
      " 0.9998743  0.99999947]\n"
     ]
    }
   ],
   "source": [
    "bits = 16  # Quantize to 8 bits\n",
    "\n",
    "quantized = uniform_scalar_quantize(original_embeddings, bits)\n",
    "dequantized = uniform_scalar_dequantize(quantized, bits, original_embeddings.min(), original_embeddings.max())\n",
    "\n",
    "print_comparison(original_embeddings, quantized, dequantized)\n",
    "#print(\"Original:\", original_embeddings[0], \"Memory Usage: \", sys.getsizeof(original_embeddings[0]))\n",
    "#print(\"Quantized:\", quantized[0], \"Memory Usage: \", sys.getsizeof(quantized[0]))\n",
    "#print(\"Dequantized:\", dequantized[0])\n",
    "\n",
    "#print(\"\\nSimilarity, Original vs. Dequantized:\", np.mean(np.cos(numpy.array(original_embeddings), numpy.array(dequantized))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IGNORE Non-uniform Scalar Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def kmeans_scalar_quantize(embeddings, n_clusters):\n",
    "    # Flatten the embeddings\n",
    "    flat_embeddings = embeddings.reshape(-1, 1)\n",
    "    \n",
    "    # Perform k-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10)\n",
    "    kmeans.fit(flat_embeddings)\n",
    "    \n",
    "    # Quantize the embeddings\n",
    "    quantized = kmeans.predict(flat_embeddings).reshape(embeddings.shape)\n",
    "    \n",
    "    return quantized, kmeans.cluster_centers_\n",
    "\n",
    "def kmeans_scalar_dequantize(quantized, cluster_centers):\n",
    "    return cluster_centers[quantized].reshape(quantized.shape)\n",
    "\n",
    "# Example usage\n",
    "n_clusters = 256  # 8-bit quantization\n",
    "\n",
    "quantized, cluster_centers = kmeans_scalar_quantize(original_embeddings, n_clusters)\n",
    "dequantized = kmeans_scalar_dequantize(quantized, cluster_centers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [ 0.00393834233885761, -0.035241456162213435, -0.035581467904314476, -0.00575613596354904 ... -0.004695540596792066, 0.040802999896525756, 0.0049649511614913875, 0.008843208585659732] Memory Usage:  112\n",
      "Quantized: [ 4, 103, 10, 228 ... 97, 64, 214, 27] Memory Usage:  112\n",
      "Dequantized: [ 0.0039102465223891745, -0.03498032376666863, -0.036069270532962446, -0.005824379986281493 ... -0.004485301835488328, 0.04063635490584795, 0.004711785236143582, 0.008579395170028216] Memory Usage:  112\n",
      "\n",
      "Similarity, Original vs. Dequantized: [0.99999999 0.99991281 0.99960331 0.99997372 0.99928661 0.9999972\n",
      " 0.9994595  0.99616831 0.99829246 0.99985146 0.99995043 0.99936638\n",
      " 0.99795224 0.99997967 0.99992438 0.99947543 0.99995011 0.99997784\n",
      " 0.99865064 0.99996072 0.99300141 1.         0.9997298  0.999988\n",
      " 0.99993678 0.99969668 0.9992162  0.99984406 0.99996161 0.99928171\n",
      " 0.99998916 0.99904102 0.99870128 0.99965787 0.9999936  0.99992066\n",
      " 0.99980722 0.9973062  0.99999533 0.99474695 0.99999291 0.99999911\n",
      " 0.99984901 0.99977668 0.99999062 0.99758872 0.99992721 0.99980763\n",
      " 0.99964051 0.99995712 0.99983146 0.99991287 0.9980452  0.99981915\n",
      " 0.99980375 0.99984486 0.99967017 0.99994511 0.99988432 0.99938508\n",
      " 0.99987107 0.99888586 0.99995007 0.99997203 0.99954493 0.99934602\n",
      " 0.99945727 0.9999896  0.99996956 0.9999831  0.99907437 0.99951522\n",
      " 0.99956287 0.99815839 0.99995713 0.9992557  0.99978024 0.99999963\n",
      " 0.99929735 0.9998258  0.99999538 0.99952281 0.99999401 0.99939243\n",
      " 0.99999838 0.99904109 0.99999594 0.99910444 0.99997761 0.99970467\n",
      " 0.99991696 0.99891278 0.81328324 0.99964497 0.99998963 0.99999582\n",
      " 0.99976115 0.9999111  0.99962871 0.99738936 0.99989899 0.99953514\n",
      " 0.99943569 0.99988823 0.99967915 0.99860217 0.9965362  0.99970001\n",
      " 0.99995802 0.99971038 0.99983923 0.9991163  0.99962819 0.99991873\n",
      " 0.99999967 0.99989478 0.99894167 0.99798532 0.99943052 0.99999418\n",
      " 0.99982417 0.99823741 0.99999666 0.99879802 0.99886104 0.99936651\n",
      " 0.99995318 0.99999978 0.99996364 0.99983122 0.998927   0.9992958\n",
      " 0.99994535 0.99632688 0.99761674 0.99965865 0.99999192 0.99981058\n",
      " 0.99958252 0.999661   0.99964287 0.99999938 0.99996936 0.99988897\n",
      " 0.99995386 0.99979647 0.99969761 0.99986321 0.99996739 0.99971913\n",
      " 0.99967712 0.99988615 0.99887464 0.99669167 0.99999834 0.99997462\n",
      " 0.99970486 0.99885004 0.99996931 0.99945474 0.99645888 0.99990635\n",
      " 0.99967832 0.99986237 0.99967383 0.99997062 0.99992585 0.99942433\n",
      " 0.99999522 0.99930228 0.99988452 0.99931331 0.99971485 0.99885573\n",
      " 0.99884919 0.99938418 0.99906938 0.99999533 0.99950345 0.99982835\n",
      " 0.99996275 0.99959166 0.9997694  0.99969627 0.99999604 0.99936616\n",
      " 0.99892064 0.99945361 0.99657091 0.9988893  0.99935621 0.99763933\n",
      " 0.99970701 0.9999987  0.9982195  0.99998607 0.99943846 0.99954466\n",
      " 0.99978281 0.99723491 0.99971554 0.99979786 0.99995812 0.99998334\n",
      " 0.99959607 0.99990352 0.99999938 0.99913698 0.99987455 0.99999686\n",
      " 0.99998056 0.99925284 0.99950808 0.99967856 0.99992746 0.99988915\n",
      " 0.99974316 0.99989813 0.99985238 0.99999428 0.99952313 0.99985902\n",
      " 0.99960102 0.99902714 0.9992078  0.99904457 0.99998111 0.99993594\n",
      " 0.9998413  0.99968758 0.99998203 0.99957079 0.99765832 0.99847391\n",
      " 0.99998783 0.99603744 0.99886667 0.99996857 0.99958348 0.99976977\n",
      " 0.99992615 0.99997038 0.99998845 0.99827794 0.99984202 0.99998175\n",
      " 0.99998384 0.99833723 0.99986777 0.99987253 0.99994029 0.99786185\n",
      " 0.99989369 0.99979655 0.99997357 0.99836656 0.99960812 0.99883854\n",
      " 0.98560934 0.99999662 0.99998277 0.99993812 0.99945861 0.99655122\n",
      " 0.99995207 0.99996912 0.99906765 0.99994319 0.99970436 0.99952267\n",
      " 0.99942266 0.99997785 0.99992417 0.99997812 0.99959999 0.9970014\n",
      " 0.99995149 0.99961841 0.99995653 0.99903679 0.99999952 0.99988375\n",
      " 0.99623203 0.99973153 0.99922549 0.99964339 0.99999471 0.99996917\n",
      " 0.99863463 0.99961088 0.99989355 0.99961001 0.99979853 0.9985729\n",
      " 0.99999291 0.99994421 0.99971001 0.99999236 0.99993407 0.99938625\n",
      " 0.99991995 0.99935409 0.99999008 0.99886881 0.9990153  0.99940468\n",
      " 0.99840092 0.99986318 0.99981664 0.99965114 0.99998392 0.99982972\n",
      " 0.99632243 0.99940859 0.99998917 0.99931151 0.99968501 0.99912625\n",
      " 0.9998786  0.99999101 0.99913224 0.99981565 0.99991872 0.99989665\n",
      " 0.99973752 0.99988498 0.99937965 0.99994958 0.99985322 0.99990556\n",
      " 0.99941553 0.99945931 0.99983641 0.99930265 0.99924521 0.9999811\n",
      " 0.99971428 0.99992199 0.99990792 0.99994945 0.99998929 0.99967328\n",
      " 0.99999783 0.99991672 0.99982389 0.99933455 0.9987639  0.99934951\n",
      " 0.99999491 0.99926562 0.99999989 0.99972176 0.99880026 0.99989707\n",
      " 0.99973442 0.99937043 0.99929169 0.99717631 0.99996456 0.99958044\n",
      " 0.99995882 0.99976365 0.99999053 0.99990805 0.99748251 0.99989164\n",
      " 0.99999664 0.99968219 0.99948903 0.99993203 0.99979395 0.99956148\n",
      " 0.99973413 0.9999994  0.99987985 0.99959309 0.99948781 0.99987824\n",
      " 0.9999348  0.99938773 0.99995977 0.99997476 0.99990683 0.99876319\n",
      " 0.99981067 0.99928737 0.99994531 0.99999284 0.99998586 0.99995002\n",
      " 0.99868137 0.99885105 0.99958304 0.99999065 0.99991391 0.99979455\n",
      " 0.99794145 0.99989611 0.99928932 0.99937273 0.99960064 0.99978363\n",
      " 0.98676073 0.99980286 0.9999982  0.99983421 0.99910842 0.99906554\n",
      " 0.99989863 0.99966526 0.99976278 0.9989927  0.99969799 0.99606684\n",
      " 0.99998759 0.99902878 0.99952236 0.99994976 0.99915155 0.99906698\n",
      " 0.99868395 0.99993542 0.99881283 0.99998134 0.99924882 0.99994365\n",
      " 0.99972809 0.99999715 0.99547028 0.99957464 0.99999686 0.99969861\n",
      " 0.99275547 0.99988898 0.99999123 0.99977567 0.99999344 0.99988808\n",
      " 0.9998752  0.99989398 0.99997343 0.99397695 0.99918424 0.99959628\n",
      " 0.99947732 0.99999979 0.99977128 0.99946063 0.9999335  0.99998359\n",
      " 0.99868493 0.99991414 0.99993429 0.99816792 0.99994426 0.99992897\n",
      " 0.99989312 0.99972227 0.99967575 0.99999379 0.99835809 0.99984255\n",
      " 0.99995328 0.99960917 0.99985287 0.99972024 0.99841805 0.99994181\n",
      " 0.9999859  0.99876545 0.99994141 0.99884092 0.99974743 1.\n",
      " 0.99838727 0.99850679 0.99990151 0.99938097 0.99999046 0.99988484\n",
      " 0.99999794 0.99933159 0.99999463 0.99896959 0.99977299 0.99924103\n",
      " 0.99904299 0.99953036 0.99992028 1.         0.99999888 0.99988297\n",
      " 0.99970472 0.99988495 0.99995571 0.99959658 0.9999992  0.9988687\n",
      " 0.99982322 0.9994368  0.99999853 0.99992258 0.99914016 0.998942\n",
      " 0.99756303 0.99999142 0.99995964 0.99998167 0.99936346 0.99959967\n",
      " 0.9998743  0.99999947]\n"
     ]
    }
   ],
   "source": [
    "print_comparison(original_embeddings, quantized, dequantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Binary Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just use 0 or support passing in threshold so median can be passed in - Simple Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def binary_quantize_threshold(embeddings, threshold=None):\n",
    "    if threshold is None:\n",
    "        threshold = np.median(embeddings)\n",
    "    return (embeddings > threshold).astype(np.uint8)\n",
    "\n",
    "def binary_dequantize_threshold(binary_embeddings, original_min, original_max):\n",
    "    return np.where(binary_embeddings, original_max, original_min)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "#1: Midpoint = 0\n",
    "\n",
    "\n",
    "#2 Midpoint = median\n",
    "binary_quantized = binary_quantize(original_embeddings)\n",
    "binary_dequantized = binary_dequantize(binary_quantized, \n",
    "                                       original_embeddings.min(), \n",
    "                                       original_embeddings.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [ 0.00393834233885761, -0.035241456162213435, -0.035581467904314476, -0.00575613596354904 ... -0.004695540596792066, 0.040802999896525756, 0.0049649511614913875, 0.008843208585659732] Memory Usage:  112\n",
      "Quantized: [ 1, 0, 0, 0 ... 0, 1, 1, 1] Memory Usage:  112\n",
      "Dequantized: [ 0.329053742183538, -0.8304248375419548, -0.8304248375419548, -0.8304248375419548 ... -0.8304248375419548, 0.329053742183538, 0.329053742183538, 0.329053742183538] Memory Usage:  112\n",
      "\n",
      "Similarity, Original vs. Dequantized: [0.99999999 0.99991281 0.99960331 0.99997372 0.99928661 0.9999972\n",
      " 0.9994595  0.99616831 0.99829246 0.99985146 0.99995043 0.99936638\n",
      " 0.99795224 0.99997967 0.99992438 0.99947543 0.99995011 0.99997784\n",
      " 0.99865064 0.99996072 0.99300141 1.         0.9997298  0.999988\n",
      " 0.99993678 0.99969668 0.9992162  0.99984406 0.99996161 0.99928171\n",
      " 0.99998916 0.99904102 0.99870128 0.99965787 0.9999936  0.99992066\n",
      " 0.99980722 0.9973062  0.99999533 0.99474695 0.99999291 0.99999911\n",
      " 0.99984901 0.99977668 0.99999062 0.99758872 0.99992721 0.99980763\n",
      " 0.99964051 0.99995712 0.99983146 0.99991287 0.9980452  0.99981915\n",
      " 0.99980375 0.99984486 0.99967017 0.99994511 0.99988432 0.99938508\n",
      " 0.99987107 0.99888586 0.99995007 0.99997203 0.99954493 0.99934602\n",
      " 0.99945727 0.9999896  0.99996956 0.9999831  0.99907437 0.99951522\n",
      " 0.99956287 0.99815839 0.99995713 0.9992557  0.99978024 0.99999963\n",
      " 0.99929735 0.9998258  0.99999538 0.99952281 0.99999401 0.99939243\n",
      " 0.99999838 0.99904109 0.99999594 0.99910444 0.99997761 0.99970467\n",
      " 0.99991696 0.99891278 0.81328324 0.99964497 0.99998963 0.99999582\n",
      " 0.99976115 0.9999111  0.99962871 0.99738936 0.99989899 0.99953514\n",
      " 0.99943569 0.99988823 0.99967915 0.99860217 0.9965362  0.99970001\n",
      " 0.99995802 0.99971038 0.99983923 0.9991163  0.99962819 0.99991873\n",
      " 0.99999967 0.99989478 0.99894167 0.99798532 0.99943052 0.99999418\n",
      " 0.99982417 0.99823741 0.99999666 0.99879802 0.99886104 0.99936651\n",
      " 0.99995318 0.99999978 0.99996364 0.99983122 0.998927   0.9992958\n",
      " 0.99994535 0.99632688 0.99761674 0.99965865 0.99999192 0.99981058\n",
      " 0.99958252 0.999661   0.99964287 0.99999938 0.99996936 0.99988897\n",
      " 0.99995386 0.99979647 0.99969761 0.99986321 0.99996739 0.99971913\n",
      " 0.99967712 0.99988615 0.99887464 0.99669167 0.99999834 0.99997462\n",
      " 0.99970486 0.99885004 0.99996931 0.99945474 0.99645888 0.99990635\n",
      " 0.99967832 0.99986237 0.99967383 0.99997062 0.99992585 0.99942433\n",
      " 0.99999522 0.99930228 0.99988452 0.99931331 0.99971485 0.99885573\n",
      " 0.99884919 0.99938418 0.99906938 0.99999533 0.99950345 0.99982835\n",
      " 0.99996275 0.99959166 0.9997694  0.99969627 0.99999604 0.99936616\n",
      " 0.99892064 0.99945361 0.99657091 0.9988893  0.99935621 0.99763933\n",
      " 0.99970701 0.9999987  0.9982195  0.99998607 0.99943846 0.99954466\n",
      " 0.99978281 0.99723491 0.99971554 0.99979786 0.99995812 0.99998334\n",
      " 0.99959607 0.99990352 0.99999938 0.99913698 0.99987455 0.99999686\n",
      " 0.99998056 0.99925284 0.99950808 0.99967856 0.99992746 0.99988915\n",
      " 0.99974316 0.99989813 0.99985238 0.99999428 0.99952313 0.99985902\n",
      " 0.99960102 0.99902714 0.9992078  0.99904457 0.99998111 0.99993594\n",
      " 0.9998413  0.99968758 0.99998203 0.99957079 0.99765832 0.99847391\n",
      " 0.99998783 0.99603744 0.99886667 0.99996857 0.99958348 0.99976977\n",
      " 0.99992615 0.99997038 0.99998845 0.99827794 0.99984202 0.99998175\n",
      " 0.99998384 0.99833723 0.99986777 0.99987253 0.99994029 0.99786185\n",
      " 0.99989369 0.99979655 0.99997357 0.99836656 0.99960812 0.99883854\n",
      " 0.98560934 0.99999662 0.99998277 0.99993812 0.99945861 0.99655122\n",
      " 0.99995207 0.99996912 0.99906765 0.99994319 0.99970436 0.99952267\n",
      " 0.99942266 0.99997785 0.99992417 0.99997812 0.99959999 0.9970014\n",
      " 0.99995149 0.99961841 0.99995653 0.99903679 0.99999952 0.99988375\n",
      " 0.99623203 0.99973153 0.99922549 0.99964339 0.99999471 0.99996917\n",
      " 0.99863463 0.99961088 0.99989355 0.99961001 0.99979853 0.9985729\n",
      " 0.99999291 0.99994421 0.99971001 0.99999236 0.99993407 0.99938625\n",
      " 0.99991995 0.99935409 0.99999008 0.99886881 0.9990153  0.99940468\n",
      " 0.99840092 0.99986318 0.99981664 0.99965114 0.99998392 0.99982972\n",
      " 0.99632243 0.99940859 0.99998917 0.99931151 0.99968501 0.99912625\n",
      " 0.9998786  0.99999101 0.99913224 0.99981565 0.99991872 0.99989665\n",
      " 0.99973752 0.99988498 0.99937965 0.99994958 0.99985322 0.99990556\n",
      " 0.99941553 0.99945931 0.99983641 0.99930265 0.99924521 0.9999811\n",
      " 0.99971428 0.99992199 0.99990792 0.99994945 0.99998929 0.99967328\n",
      " 0.99999783 0.99991672 0.99982389 0.99933455 0.9987639  0.99934951\n",
      " 0.99999491 0.99926562 0.99999989 0.99972176 0.99880026 0.99989707\n",
      " 0.99973442 0.99937043 0.99929169 0.99717631 0.99996456 0.99958044\n",
      " 0.99995882 0.99976365 0.99999053 0.99990805 0.99748251 0.99989164\n",
      " 0.99999664 0.99968219 0.99948903 0.99993203 0.99979395 0.99956148\n",
      " 0.99973413 0.9999994  0.99987985 0.99959309 0.99948781 0.99987824\n",
      " 0.9999348  0.99938773 0.99995977 0.99997476 0.99990683 0.99876319\n",
      " 0.99981067 0.99928737 0.99994531 0.99999284 0.99998586 0.99995002\n",
      " 0.99868137 0.99885105 0.99958304 0.99999065 0.99991391 0.99979455\n",
      " 0.99794145 0.99989611 0.99928932 0.99937273 0.99960064 0.99978363\n",
      " 0.98676073 0.99980286 0.9999982  0.99983421 0.99910842 0.99906554\n",
      " 0.99989863 0.99966526 0.99976278 0.9989927  0.99969799 0.99606684\n",
      " 0.99998759 0.99902878 0.99952236 0.99994976 0.99915155 0.99906698\n",
      " 0.99868395 0.99993542 0.99881283 0.99998134 0.99924882 0.99994365\n",
      " 0.99972809 0.99999715 0.99547028 0.99957464 0.99999686 0.99969861\n",
      " 0.99275547 0.99988898 0.99999123 0.99977567 0.99999344 0.99988808\n",
      " 0.9998752  0.99989398 0.99997343 0.99397695 0.99918424 0.99959628\n",
      " 0.99947732 0.99999979 0.99977128 0.99946063 0.9999335  0.99998359\n",
      " 0.99868493 0.99991414 0.99993429 0.99816792 0.99994426 0.99992897\n",
      " 0.99989312 0.99972227 0.99967575 0.99999379 0.99835809 0.99984255\n",
      " 0.99995328 0.99960917 0.99985287 0.99972024 0.99841805 0.99994181\n",
      " 0.9999859  0.99876545 0.99994141 0.99884092 0.99974743 1.\n",
      " 0.99838727 0.99850679 0.99990151 0.99938097 0.99999046 0.99988484\n",
      " 0.99999794 0.99933159 0.99999463 0.99896959 0.99977299 0.99924103\n",
      " 0.99904299 0.99953036 0.99992028 1.         0.99999888 0.99988297\n",
      " 0.99970472 0.99988495 0.99995571 0.99959658 0.9999992  0.9988687\n",
      " 0.99982322 0.9994368  0.99999853 0.99992258 0.99914016 0.998942\n",
      " 0.99756303 0.99999142 0.99995964 0.99998167 0.99936346 0.99959967\n",
      " 0.9998743  0.99999947]\n"
     ]
    }
   ],
   "source": [
    "print_comparison(original_embeddings, binary_quantized, binary_dequantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IGNORE - Iterative Quantization (ITQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITQ Quantized: [1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1.\n",
      " 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0.\n",
      " 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0.\n",
      " 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1.\n",
      " 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0.\n",
      " 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1.\n",
      " 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0.\n",
      " 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1.\n",
      " 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 0. 1. 1. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "def itq(data, num_iterations=50):\n",
    "    # Center the data\n",
    "    data = data - np.mean(data, axis=0)\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=data.shape[1])\n",
    "    data = pca.fit_transform(data)\n",
    "    \n",
    "    # Initialize random rotation\n",
    "    R = np.random.randn(data.shape[1], data.shape[1])\n",
    "    U, _, Vt = np.linalg.svd(R)\n",
    "    R = U.dot(Vt)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Fix R and update B\n",
    "        Z = np.dot(data, R)\n",
    "        B = np.sign(Z)\n",
    "        \n",
    "        # Fix B and update R\n",
    "        UB, _, UAT = np.linalg.svd(np.dot(data.T, B))\n",
    "        R = np.dot(UB, UAT)\n",
    "    \n",
    "    # Final binary codes\n",
    "    Z = np.dot(data, R)\n",
    "    B = np.sign(Z)\n",
    "    \n",
    "    return (B + 1) / 2  # Convert to 0 and 1\n",
    "\n",
    "# Example usage\n",
    "itq_quantized = itq(original_embeddings)\n",
    "print(\"ITQ Quantized:\", itq_quantized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/var/cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting nanopq\n",
      "  Downloading nanopq-0.2.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from nanopq) (1.23.5)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from nanopq) (1.10.0)\n",
      "Downloading nanopq-0.2.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: nanopq\n",
      "Successfully installed nanopq-0.2.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install nanopq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_embeddings.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "M: 8, Ks: 256, metric : <class 'numpy.uint8'>, code_dtype: l2\n",
      "iter: 20, seed: 0\n",
      "Training the subspace: 0 / 8\n",
      "Training the subspace: 1 / 8\n",
      "Training the subspace: 2 / 8\n",
      "Training the subspace: 3 / 8\n",
      "Training the subspace: 4 / 8\n",
      "Training the subspace: 5 / 8\n",
      "Training the subspace: 6 / 8\n",
      "Training the subspace: 7 / 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<nanopq.pq.PQ at 0x7f6549a06020>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nanopq\n",
    "import numpy as np\n",
    "\n",
    "embeddings = numpy.array(original_embeddings, dtype=np.float32)\n",
    "\n",
    "N = embeddings.shape[0]\n",
    "D = embeddings.shape[1]\n",
    "M=8\n",
    "X = numpy.array(original_embeddings, dtype=np.float32)\n",
    "print(X.dtype)\n",
    "\n",
    "#N, Nt, D = 10000, 2000, 128\n",
    "#X = np.random.random((N, D)).astype(np.float32)  # 10,000 128-dim vectors to be indexed\n",
    "#Xt = np.random.random((Nt, D)).astype(np.float32)  # 2,000 128-dim vectors for training\n",
    "\n",
    "#query = np.random.random((D,)).astype(np.float32)  # a 128-dim query vector\n",
    "\n",
    "# Instantiate with M=8 sub-spaces\n",
    "pq = nanopq.PQ(M=M)\n",
    "\n",
    "# Train codewords\n",
    "pq.fit(X, seed=0) #seed for data consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding the subspace: 0 / 8\n",
      "Encoding the subspace: 1 / 8\n",
      "Encoding the subspace: 2 / 8\n",
      "Encoding the subspace: 3 / 8\n",
      "Encoding the subspace: 4 / 8\n",
      "Encoding the subspace: 5 / 8\n",
      "Encoding the subspace: 6 / 8\n",
      "Encoding the subspace: 7 / 8\n",
      "[0.14254893 0.70412374 0.7292688  0.23975845 0.57965636 0.7138767\n",
      " 0.56706476 0.7499395  0.74030113 0.6424299 ]\n"
     ]
    }
   ],
   "source": [
    "# Encode to PQ-codes\n",
    "quantized_documents = pq.encode(X)  # (10000, 8) with dtype=np.uint8\n",
    "#print(quantized)\n",
    "\n",
    "#query = pq.encode(embeddings[0]) #just grab the first movie embedding\n",
    "query = embeddings[0]\n",
    "\n",
    "# Results: create a distance table online, and compute Asymmetric Distance to each PQ-code \n",
    "dists = pq.dtable(query).adist(quantized_documents)  # (10000, )\n",
    "\n",
    "#TODO: sort results\n",
    "print(dists[:10]) #not currently sorted. I think these may be in order of cluster id. If so, will need a map of id:score and then to sort by score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matroyoshka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SentenceTransformer.__init__() got an unexpected keyword argument 'truncate_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[250], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m matryoshka_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#model = SentenceTransformer(\"tomaarsen/mpnet-base-nli-matryoshka\", truncate_dim=matryoshka_dim)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmixedbread-ai/mxbai-embed-large-v1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatryoshka_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(\n\u001b[1;32m      9\u001b[0m     [\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe weather is so nice!\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     ]\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(embeddings\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mTypeError\u001b[0m: SentenceTransformer.__init__() got an unexpected keyword argument 'truncate_dim'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "matryoshka_dim = 64\n",
    "#model = SentenceTransformer(\"tomaarsen/mpnet-base-nli-matryoshka\", truncate_dim=matryoshka_dim)\n",
    "model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\", truncate_dim=matryoshka_dim)\n",
    "\n",
    "embeddings = model.encode(\n",
    "    [\n",
    "        \"The weather is so nice!\",\n",
    "        \"It's so sunny outside!\",\n",
    "        \"He drove to the stadium.\",\n",
    "    ]\n",
    ")\n",
    "print(embeddings.shape)\n",
    "# => (3, 64)\n",
    "\n",
    "# Similarity of the first sentence to the other two:\n",
    "similarities = cos_sim(embeddings[0], embeddings[1:])\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IGNORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (1000,102,102) into shape (1000,102)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[189], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m bits_per_subvector \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m     42\u001b[0m pq_quantized, codebooks \u001b[38;5;241m=\u001b[39m product_quantize(original_embeddings, num_subvectors, bits_per_subvector)\n\u001b[0;32m---> 43\u001b[0m pq_dequantized \u001b[38;5;241m=\u001b[39m \u001b[43mproduct_dequantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpq_quantized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodebooks\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[189], line 34\u001b[0m, in \u001b[0;36mproduct_dequantize\u001b[0;34m(quantized, codebooks)\u001b[0m\n\u001b[1;32m     32\u001b[0m     start \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m*\u001b[39m subvector_size\n\u001b[1;32m     33\u001b[0m     end \u001b[38;5;241m=\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m subvector_size\n\u001b[0;32m---> 34\u001b[0m     \u001b[43mdequantized\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m codebooks[i][quantized[:, start:end]]\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dequantized\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (1000,102,102) into shape (1000,102)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def product_quantize(embeddings, num_subvectors, bits_per_subvector):\n",
    "    d = embeddings.shape[1]\n",
    "    subvector_size = d // num_subvectors\n",
    "    num_centroids = 2**bits_per_subvector\n",
    "    \n",
    "    quantized = np.zeros_like(embeddings, dtype=np.uint8)\n",
    "    codebooks = []\n",
    "    \n",
    "    for i in range(num_subvectors):\n",
    "        start = i * subvector_size\n",
    "        end = (i + 1) * subvector_size\n",
    "        subvector = embeddings[:, start:end]\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=num_centroids, n_init=10)\n",
    "        kmeans.fit(subvector)\n",
    "        \n",
    "        quantized[:, start:end] = kmeans.predict(subvector).reshape(-1, 1)\n",
    "        codebooks.append(kmeans.cluster_centers_)\n",
    "    \n",
    "    return quantized, codebooks\n",
    "\n",
    "def product_dequantize(quantized, codebooks):\n",
    "    num_subvectors = len(codebooks)\n",
    "    subvector_size = codebooks[0].shape[1]\n",
    "    \n",
    "    dequantized = np.zeros((quantized.shape[0], num_subvectors * subvector_size))\n",
    "    \n",
    "    for i in range(num_subvectors):\n",
    "        start = i * subvector_size\n",
    "        end = (i + 1) * subvector_size\n",
    "        dequantized[:, start:end] = codebooks[i][quantized[:, start:end]]\n",
    "    \n",
    "    return dequantized\n",
    "\n",
    "# Example usage\n",
    "num_subvectors = 5\n",
    "bits_per_subvector = 8\n",
    "\n",
    "pq_quantized, codebooks = product_quantize(original_embeddings, num_subvectors, bits_per_subvector)\n",
    "pq_dequantized = product_dequantize(pq_quantized, codebooks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(\"Original:\", original_embeddings[0])\n",
    "#print(\"PQ Quantized:\", pq_quantized[0])\n",
    "#print(\"PQ Dequantized:\", pq_dequantized[0])\n",
    "\n",
    "#print(\"Similarity, Original vs. Dequantized:\", np.dot(original_embeddings[0], pq_dequantized[0]))\n",
    "print_comparisons(original_embeddings, pq_quantized, pq_dequantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def fit_itq(data, num_iterations=50):\n",
    "    # Center the data\n",
    "    mean = np.mean(data, axis=0)\n",
    "    centered_data = data - mean\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=data.shape[1])\n",
    "    data_pca = pca.fit_transform(centered_data)\n",
    "    \n",
    "    # Initialize random rotation\n",
    "    R = np.random.randn(data_pca.shape[1], data_pca.shape[1])\n",
    "    U, _, Vt = np.linalg.svd(R)\n",
    "    R = U.dot(Vt)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Fix R and update B\n",
    "        Z = np.dot(data_pca, R)\n",
    "        B = np.sign(Z)\n",
    "        \n",
    "        # Fix B and update R\n",
    "        UB, _, UAT = np.linalg.svd(np.dot(data_pca.T, B))\n",
    "        R = np.dot(UB, UAT)\n",
    "    \n",
    "    return R, mean, pca\n",
    "\n",
    "def itq_quantize(data, R, mean, pca):\n",
    "    # Center and project the data\n",
    "    centered_data = data - mean\n",
    "    data_pca = pca.transform(centered_data)\n",
    "    \n",
    "    # Apply rotation and binarize\n",
    "    Z = np.dot(data_pca, R)\n",
    "    B = np.sign(Z)\n",
    "    \n",
    "    return (B + 1) / 2  # Convert to 0 and 1\n",
    "\n",
    "def itq_dequantize(binary_codes, R, mean, pca):\n",
    "    # Convert back to -1 and 1\n",
    "    B = 2 * binary_codes - 1\n",
    "    \n",
    "    # Inverse rotation\n",
    "    Z_approx = np.dot(B, R.T)\n",
    "    \n",
    "    # Inverse PCA\n",
    "    data_approx = pca.inverse_transform(Z_approx)\n",
    "    \n",
    "    # Add back the mean\n",
    "    return data_approx + mean\n",
    "\n",
    "# Example usage\n",
    "#original_embeddings = np.random.rand(100, 64)  # 100 embeddings of dimension 64\n",
    "\n",
    "# Fit ITQ\n",
    "R, mean, pca = fit_itq(original_embeddings, num_iterations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quantize\n",
    "itq_quantized = itq_quantize(original_embeddings, R, mean, pca)\n",
    "\n",
    "# Dequantize\n",
    "itq_dequantized = itq_dequantize(itq_quantized, R, mean, pca)\n",
    "\n",
    "# Print results for the first embedding\n",
    "#print(\"Original:\", original_embeddings[0])\n",
    "#print(\"ITQ Quantized:\", itq_quantized[0])\n",
    "#print(\"ITQ Dequantized:\", itq_dequantized[0])\n",
    "\n",
    "# Compute and print mean squared error\n",
    "#mse = np.mean((original_embeddings - itq_dequantized) ** 2)\n",
    "#print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "print_comparison(original_embeddings, itq_quantized, itq_dequantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/var/cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -uggingface-hub (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting sentence-transformers==2.7.0\n",
      "  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting transformers<5.0.0,>=4.34.0 (from sentence-transformers==2.7.0)\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.7.0) (4.64.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.7.0) (2.4.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.7.0) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.7.0) (1.2.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.7.0) (1.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.7.0) (0.17.3)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.7.0) (9.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers==2.7.0) (3.15.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers==2.7.0) (2024.6.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers==2.7.0) (2.28.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers==2.7.0) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers==2.7.0) (4.12.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers==2.7.0) (23.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers==2.7.0) (12.5.82)\n",
      "Collecting huggingface-hub>=0.15.1 (from sentence-transformers==2.7.0)\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers==2.7.0) (2024.7.24)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers==2.7.0) (0.4.4)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers<5.0.0,>=4.34.0->sentence-transformers==2.7.0)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==2.7.0) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==2.7.0) (3.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers==2.7.0) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers==2.7.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers==2.7.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers==2.7.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers==2.7.0) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers==2.7.0) (1.2.1)\n",
      "Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -uggingface-hub (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.17.3\n",
      "    Uninstalling huggingface-hub-0.17.3:\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: 'top_level.txt'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers==2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'quantization' from 'sentence_transformers' (/opt/conda/lib/python3.10/site-packages/sentence_transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[208], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quantization\n\u001b[1;32m      4\u001b[0m binary_embeddings \u001b[38;5;241m=\u001b[39m quantize_embeddings(embeddings, precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'quantization' from 'sentence_transformers' (/opt/conda/lib/python3.10/site-packages/sentence_transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.quantization import quantize_embeddings\n",
    "\n",
    "binary_embeddings = quantize_embeddings(embeddings, precision=\"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IGNORE: Locally-adaptive quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LAVQ:\n",
    "    def __init__(self, codebook_size, learning_rate=0.1):\n",
    "        self.codebook_size = codebook_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.codebook = None\n",
    "    \n",
    "    def fit(self, data):\n",
    "        # Initialize codebook with random samples from the data\n",
    "        self.codebook = data[np.random.choice(data.shape[0], self.codebook_size, replace=False)]\n",
    "    \n",
    "    def quantize(self, data):\n",
    "        quantized = np.zeros(data.shape[0], dtype=int)\n",
    "        for i, vector in enumerate(data):\n",
    "            # Find the closest codebook vector\n",
    "            distances = np.sum((self.codebook - vector) ** 2, axis=1)\n",
    "            closest_index = np.argmin(distances)\n",
    "            quantized[i] = closest_index\n",
    "            \n",
    "            # Update the closest codebook vector\n",
    "            self.codebook[closest_index] += self.learning_rate * (vector - self.codebook[closest_index])\n",
    "        \n",
    "        return quantized\n",
    "    \n",
    "    def dequantize(self, quantized):\n",
    "        return self.codebook[quantized]\n",
    "\n",
    "# Example usage\n",
    "np.random.seed(42)  # for reproducibility\n",
    "original_embeddings = np.random.rand(1000, 10)  # 1000 embeddings of dimension 10\n",
    "\n",
    "lavq = LAVQ(codebook_size=256)  # 8-bit quantization\n",
    "lavq.fit(original_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [0.37454012 0.95071431 0.73199394 0.59865848 0.15601864 0.15599452\n",
      " 0.05808361 0.86617615 0.60111501 0.70807258]\n",
      "Quantized Index: 80\n",
      "Dequantized: [0.32761169 0.87076841 0.76142373 0.62190297 0.17016318 0.14367201\n",
      " 0.10737511 0.85391109 0.53493335 0.713197  ]\n",
      "Mean Squared Error: 0.021941625251251977\n"
     ]
    }
   ],
   "source": [
    "# Quantize the embeddings\n",
    "quantized = lavq.quantize(original_embeddings)\n",
    "\n",
    "# Dequantize\n",
    "dequantized = lavq.dequantize(quantized)\n",
    "\n",
    "# Print results for the first embedding\n",
    "print(\"Original:\", original_embeddings[0])\n",
    "print(\"Quantized Index:\", quantized[0])\n",
    "print(\"Dequantized:\", dequantized[0])\n",
    "\n",
    "# Compute and print mean squared error\n",
    "mse = np.mean((original_embeddings - dequantized) ** 2)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
